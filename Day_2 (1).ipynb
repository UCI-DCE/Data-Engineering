{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J_9TOllsNt2"
      },
      "source": [
        "## `Mount Google Drive`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc6idE1ipdy0",
        "outputId": "c0bb6b4e-a6d5-464e-96a1-3e877112379e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "UVoe2U0soQL2"
      },
      "outputs": [],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#/dev/null:to avoid verbose\n",
        "\n",
        "# Download and extract Hadoop\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
        "!tar -xzf hadoop-3.4.0.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkO0WSTHRDb0"
      },
      "source": [
        "## `Unzip the hadoop 3.4.0 to the directory`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYFMs4BvuS0G"
      },
      "outputs": [],
      "source": [
        "# Extract Hadoop to the directory\n",
        "#!tar -xzf hadoop-3.4.0.tar.gz -C /content/drive/MyDrive/hadoopcolab\n",
        "#if you want to change the unzipped file to a different directory, very slow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0umqzVkp2i-"
      },
      "source": [
        "### Set Hadoop Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjIqtuW4prO9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the path to your Hadoop installation\n",
        "hadoop_path = '/content/hadoop-3.4.0'\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['HADOOP_HOME'] = hadoop_path\n",
        "#os.environ['PATH'] = os.environ['PATH'] + f':{hadoop_path}/bin:{hadoop_path}/sbin'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsf8kEkBRwEV"
      },
      "source": [
        "## Create a directory in hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW25qoPO1usK"
      },
      "outputs": [],
      "source": [
        "!$hadoop_path/bin/hadoop fs -mkdir -p /content/drive/MyDrive/My_Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Sp6piRsQUH"
      },
      "outputs": [],
      "source": [
        "!$hadoop_path/bin/hadoop fs -ls /content/drive/MyDrive/My_Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr8r_aqJsxRu"
      },
      "outputs": [],
      "source": [
        "hd_path = '/content/hadoop-3.4.0/bin/hadoop'\n",
        "dir_path = '/content/drive/MyDrive/My_Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itge6J4JtF48"
      },
      "outputs": [],
      "source": [
        "!$hd_path fs  -touchz $dir_path/file.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOx73Ns_ohNq",
        "outputId": "9f71483b-d275-4958-a63c-52085e92a4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/My_Data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/My_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s_VzmYE1UPz",
        "outputId": "33134eac-b4fd-4532-f966-210656dd1470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTkDP6od0kne"
      },
      "outputs": [],
      "source": [
        "!$hd_path fs -cat /content/drive/MyDrive/My_Data/file.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_k8CRZ4zidM",
        "outputId": "6459ca3f-fc39-41ff-95e2-12bb9d797647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=\"house, dog, cat, rat, house, bee\"\n",
        "# Create a temporary file and append new data\n",
        "temp_file=\"temp_file.txt\"\n",
        "!echo \"$new_data\" >> \"$temp_file\"\n",
        "# Remove existing file and upload the updated file\n",
        "!$hd_path fs -rm \"$dir_path/file.txt\"\n",
        "!$hd_path fs -put -f \"$temp_file\" \"$dir_path/file.txt\"\n",
        "# Clean up temporary file\n",
        "!rm \"$temp_file\"\n",
        "##will throw me conf infor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfBQayXDyJ1S",
        "outputId": "e85c9ccf-2c2c-4294-f9d6-9258ed842bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-22 19:37:41,697 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/drive/MyDrive/My_Data/file.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmSSZmdqz6qY",
        "outputId": "c0481473-a648-4865-9918-08501afbf157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "house, dog, cat, rat, house, bee\n"
          ]
        }
      ],
      "source": [
        "!$hd_path fs -cat \"$dir_path/file.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2y-1VUAYCSR",
        "outputId": "a462f674-e763-4448-90c1-628741af1a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-------   1 root root         33 2024-05-22 19:37 file.txt\n",
            "-rw-------   1 root root  965537117 2024-03-04 09:36 hadoop-3.4.0.tar.gz\n"
          ]
        }
      ],
      "source": [
        "hdfs_path = '/content/hadoop-3.4.0/bin/'\n",
        "!$hdfs_path/hdfs dfs -ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVA6E2ul2Mzi",
        "outputId": "8afefd35-41fa-4cef-9e2a-8c1e0315dcde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-------   1 root root         33 2024-05-22 19:37 file.txt\n",
            "-rw-------   1 root root  965537117 2024-03-04 09:36 hadoop-3.4.0.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!$hdfs_path/hdfs dfs -ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOzBzPPu88Q6",
        "outputId": "d16b93d6-82de-4881-96b7-10174be3080b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdfs\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m743.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from hdfs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2024.2.2)\n",
            "Building wheels for collected packages: hdfs, docopt\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=1db39fc8ae63db17421ac576491baf16ce4160a98a5ffb349bab696d427f554d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6fe0fe034be03c0ba210daeec9febe46a862860dcd1b8c960d363509396a4df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built hdfs docopt\n",
            "Installing collected packages: docopt, hdfs\n",
            "Successfully installed docopt-0.6.2 hdfs-2.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc24oGusGF5s"
      },
      "outputs": [],
      "source": [
        "# Create the 'sample_data' directory in HDFS\n",
        "! {hd_path} fs -mkdir /mydata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfiQ_YDRSYdx"
      },
      "source": [
        "## `Convert word file to txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiA9K_wZ4-0e",
        "outputId": "ea9b1404-c7d1-4849-cb03-5bfd1ae2d488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAF1nCWLOvNZ"
      },
      "outputs": [],
      "source": [
        "import docx\n",
        "local_file_path = '/content/drive/MyDrive/HW6_Kowarsch.docx'\n",
        "\n",
        "doc = docx.Document(local_file_path)\n",
        "text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "with open('/content/drive/MyDrive/HW6_Kowarsch.txt', 'w') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT6HVix0O1To"
      },
      "outputs": [],
      "source": [
        "local_txt_file = '/content/drive/MyDrive/HW6_Kowarsch.txt'\n",
        "hdfs_txt_file = '/mydata/HW6_Kowarsch.txt'\n",
        "! {hd_path} fs -put -f $local_txt_file $hdfs_txt_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDd5CS3Pfsp",
        "outputId": "7f6a1c1a-a350-4f96-c741-6f8f5c2ef32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0d0d9b59c902d5f82b4c0b8ea9e2ea238464c2e20510575fd455472c44701038\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnP4lXfuPduU",
        "outputId": "9e10209e-5a11-4a07-d02c-f3a8fb38dcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|word   |count|\n",
            "+-------+-----+\n",
            "|the    |76   |\n",
            "|       |71   |\n",
            "|of     |47   |\n",
            "|to     |30   |\n",
            "|is     |26   |\n",
            "|a      |26   |\n",
            "|in     |20   |\n",
            "|The    |16   |\n",
            "|that   |16   |\n",
            "|be     |13   |\n",
            "|and    |12   |\n",
            "|privacy|12   |\n",
            "|query  |10   |\n",
            "|,      |9    |\n",
            "|data   |9    |\n",
            "|patient|9    |\n",
            "|for    |8    |\n",
            "|on     |8    |\n",
            "|or     |8    |\n",
            "|set    |7    |\n",
            "+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Clear the Spark session\n",
        "spark.stop()\n",
        "#reconnect the session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the text file from HDFS\n",
        "hdfs_txt_file = '/mydata/HW6_Kowarsch.txt' #replace this file with your own txt.file\n",
        "df = spark.read.text(hdfs_txt_file)\n",
        "\n",
        "# Perform word count\n",
        "word_count = (df\n",
        "              .select(\"value\")\n",
        "              .withColumn(\"words\", f.split(\"value\", \" \"))\n",
        "              .select(\"words\")\n",
        "              .withColumn(\"word\", f.explode(\"words\"))\n",
        "              .groupBy(\"word\")\n",
        "              .count()\n",
        "              .orderBy(\"count\", ascending=False))\n",
        "\n",
        "# Display the results\n",
        "word_count.show(n=20, truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}