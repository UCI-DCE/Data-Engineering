{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J_9TOllsNt2"
      },
      "source": [
        "## `Mount Google Drive`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc6idE1ipdy0",
        "outputId": "38b9e13d-35f8-4371-8ec4-fdc992fd3494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install Java if not already installed\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Define the URL and file name for Hadoop\n",
        "hadoop_url = \"https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\"\n",
        "hadoop_file = \"hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "# Check if the Hadoop tar.gz file already exists\n",
        "if not os.path.exists(hadoop_file):\n",
        "    # Download the Hadoop tar.gz file if it doesn't exist\n",
        "    !wget -q $hadoop_url\n",
        "else:\n",
        "    print(\"Hadoop tar.gz file already exists. Skipping download.\")\n",
        "\n",
        "# Extract the Hadoop tar.gz file\n",
        "!tar -xzf $hadoop_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zflvykUDt922",
        "outputId": "65a842a5-a847-4e7b-b134-7ce7803d3b5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop tar.gz file already exists. Skipping download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0umqzVkp2i-"
      },
      "source": [
        "### Set Hadoop Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qjIqtuW4prO9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the path to your Hadoop installation\n",
        "hadoop_path = '/content/hadoop-3.4.0'\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['HADOOP_HOME'] = hadoop_path\n",
        "os.environ['PATH'] = os.environ['PATH'] + f':{hadoop_path}/bin:{hadoop_path}/sbin'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove existing directory and its contents\n",
        "!$hadoop_path/bin/hadoop fs -rm -r -skipTrash /content/drive/MyDrive/My_Data\n",
        "\n",
        "# Create a new empty directory\n",
        "!$hadoop_path/bin/hadoop fs -mkdir -p /content/drive/MyDrive/My_Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2D8eQGpWqfE",
        "outputId": "8067353a-05c2-4535-9ea0-ff43c1c74625"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted /content/drive/MyDrive/My_Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can also create hd_path and dir_path to access hadoop and the new directory, but it is optional\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bd7wqo9UWPFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sr8r_aqJsxRu"
      },
      "outputs": [],
      "source": [
        "hd_path = '/content/hadoop-3.4.0/bin/hadoop'\n",
        "dir_path = '/content/drive/MyDrive/My_Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sTkDP6od0kne"
      },
      "outputs": [],
      "source": [
        "!$hadoop_path/bin/hadoop fs -ls /content/drive/MyDrive/My_Data\n",
        "##shoud return nothing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a file.txt file and copy it into My_Data directory using a temp file, then remove the temp file."
      ],
      "metadata": {
        "id": "1vqdBVg9qCYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=\"house, dog, cat, rat, house, bee\"\n",
        "# Create a temporary file\n",
        "temp_file=\"file.txt\"\n",
        "!echo \"$new_data\" >> \"$temp_file\"\n",
        "# Copy the temp_file to the dir_path/file.txt\n",
        "!$hd_path fs -put -f \"$temp_file\" \"$dir_path/file.txt\"\n",
        "# Clean up temporary file\n",
        "!rm \"$temp_file\""
      ],
      "metadata": {
        "id": "JfBQayXDyJ1S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show content of the file.txt using -cat"
      ],
      "metadata": {
        "id": "ehxVLpWAhCpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmSSZmdqz6qY",
        "outputId": "757a91f4-89c8-4649-a801-c89039eac93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "house, dog, cat, rat, house, bee\n"
          ]
        }
      ],
      "source": [
        "!$hadoop_path/bin/hadoop fs -cat \"$dir_path/file.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring hdfs"
      ],
      "metadata": {
        "id": "lv-x3Vv-hIzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U2y-1VUAYCSR"
      },
      "outputs": [],
      "source": [
        "hdfs_path = '/content/hadoop-3.4.0/bin'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Python package ndfs for interacting with HDFS"
      ],
      "metadata": {
        "id": "akB43nLhjR75"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfiQ_YDRSYdx"
      },
      "source": [
        "## `Convert word file to txt`-- optional.\n",
        "-  When your input file is Word.DOC;\n",
        "- No need to do covertion if it is a .txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiA9K_wZ4-0e",
        "outputId": "db8a0fe1-5515-452d-8b5b-6093e752c19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RAF1nCWLOvNZ"
      },
      "outputs": [],
      "source": [
        "import docx\n",
        "local_file_path = '/content/drive/MyDrive/HW6_Kowarsch.docx'\n",
        "\n",
        "doc = docx.Document(local_file_path)\n",
        "text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "with open('/content/drive/MyDrive/HW6_Kowarsch.txt', 'w') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use hadoop executable to interact with HDFS, includign creating directories"
      ],
      "metadata": {
        "id": "iGsKgxjaqy24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hadoop_path = '/content/hadoop-3.4.0/bin/hdfs'  # Correct path to HDFS executable\n",
        "hdfs_path = '/HDFS_Data'  # Folder path in HDFS\n",
        "local_txt_file = '/content/drive/MyDrive/HW6_Kowarsch.txt'\n",
        "hdfs_txt_file = f'{hdfs_path}/HW6_Kowarsch.txt'\n",
        "\n",
        "# Create the directory in HDFS\n",
        "!{hadoop_path} dfs -mkdir -p {hdfs_path}\n",
        "\n",
        "# Copy the local text file to HDFS\n",
        "!{hadoop_path} dfs -put -f {local_txt_file} {hdfs_txt_file}\n"
      ],
      "metadata": {
        "id": "rw_BbMV-l8kn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDd5CS3Pfsp",
        "outputId": "ccb28794-2290-492c-e6d2-0c37d7d8f70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnP4lXfuPduU",
        "outputId": "96d59900-1d88-43c7-ee3d-03238b67486f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|word   |count|\n",
            "+-------+-----+\n",
            "|the    |76   |\n",
            "|       |71   |\n",
            "|of     |47   |\n",
            "|to     |30   |\n",
            "|is     |26   |\n",
            "|a      |26   |\n",
            "|in     |20   |\n",
            "|The    |16   |\n",
            "|that   |16   |\n",
            "|be     |13   |\n",
            "|and    |12   |\n",
            "|privacy|12   |\n",
            "|query  |10   |\n",
            "|,      |9    |\n",
            "|data   |9    |\n",
            "|patient|9    |\n",
            "|for    |8    |\n",
            "|on     |8    |\n",
            "|or     |8    |\n",
            "|set    |7    |\n",
            "+-------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Clear the Spark session\n",
        "spark.stop()\n",
        "#reconnect the session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the text file from HDFS\n",
        "hdfs_txt_file = '/HDFS_Data/HW6_Kowarsch.txt' #replace this file with your own txt.file\n",
        "df = spark.read.text(hdfs_txt_file)\n",
        "\n",
        "# Perform word count\n",
        "word_count = (df\n",
        "              .select(\"value\")\n",
        "              .withColumn(\"words\", f.split(\"value\", \" \"))\n",
        "              .select(\"words\")\n",
        "              .withColumn(\"word\", f.explode(\"words\"))\n",
        "              .groupBy(\"word\")\n",
        "              .count()\n",
        "              .orderBy(\"count\", ascending=False))\n",
        "\n",
        "# Display the results\n",
        "word_count.show(n=20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clear off all directories"
      ],
      "metadata": {
        "id": "3i1pY3BypBEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hadoop_path = '/content/hadoop-3.4.0/bin/hadoop'  # Path to Hadoop binaries\n",
        "hdfs_path = '/HDFS_Data'  # Folder path in HDFS\n",
        "local_path = '/content/drive/MyDrive/My_Data'  # Local folder path in Hadoop\n",
        "\n",
        "# Delete the directory in HDFS\n",
        "!{hadoop_path} fs -rm -r {hdfs_path}\n",
        "\n",
        "# Delete the directory in Hadoop\n",
        "!rm -rf {local_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96e67bUxoH7J",
        "outputId": "fefd8fc5-3466-4542-81d4-1a9e8d0bd228"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-22 23:51:08,053 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /HDFS_Data\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
